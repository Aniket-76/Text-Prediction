{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aniket-76/Text-Prediction/blob/main/Copy_of_Chat_bot_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3TOfcaL__3U"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C3/W4/ungraded_labs/C3_W4_Lab_2_irish_lyrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqIxQYm8h06Z"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb1mfOvch4Sv"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOwsuGQQY9OL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np \n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmBFI788pOXx"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pylt5qZYsWPh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "path='/content/drive/MyDrive/Songs_Dataset_new/New romantic'\n",
        "os.chdir(path)\n",
        "merged_data=\"\"\n",
        "for file in os.listdir():\n",
        "  if file != \"English_phrases_and_sayings.txt\":\n",
        "    f=open(file,mode='r',encoding='utf-8')\n",
        "    for line in f:\n",
        "      if not line.isspace():\n",
        "        line=line.replace(',',\"\")\n",
        "        line=line.replace('\\n',\"\")\n",
        "        line=line.replace('-',\"\")\n",
        "        line=line.replace('/',\"\")\n",
        "        merged_data=merged_data+line\n",
        "    f.close()\n",
        "    merged_data+=\"\\n\"\n",
        "print(merged_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path='/content/drive/MyDrive/Songs_Dataset_new/New romantic/English_phrases_and_sayings.txt'\n",
        "f=open(file,mode='r',encoding='utf-8')\n",
        "for line in f:\n",
        "  if not line.isspace():\n",
        "    line=line.replace(',',\"\")\n",
        "    line=line.replace('\\n',\"\")\n",
        "    line=line.replace('-',\"\")\n",
        "    line=line.replace('/',\"\")\n",
        "    tns=line.split(\" \")\n",
        "    if len(tns)>=5:\n",
        "      merged_data=merged_data+line+\"\\n\"\n",
        "f.close()\n",
        "merged_data+=\"\\n\"\n",
        "with open(\"/content/sample_data/chat_bot_data.txt\",mode='x',encoding='utf-8') as f:\n",
        "  f.write(merged_data)"
      ],
      "metadata": {
        "id": "8tjekKuQZi0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9F1f-1DjYw8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v6JYQGNPXCW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKOO7DFCPX3L"
      },
      "outputs": [],
      "source": [
        "data = open('chat-bot-data.txt').read()\n",
        "\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "print(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkP2CP0qP8RD"
      },
      "source": [
        "From here, you can initialize the `Tokenizer` class and generate the word index dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRnDnCW-Z7qv"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(f'word index dictionary: {tokenizer.word_index}')\n",
        "print(f'total words: {total_words}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK29FzZ7QW-4"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soPGVheskaQP"
      },
      "outputs": [],
      "source": [
        "input_sequences = []\n",
        "\n",
        "for line in corpus:\n",
        "\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\t\n",
        "\t\tsub_phrases = token_list[:i+1]\n",
        "\n",
        "\t\tinput_sequences.append(sub_phrases)\n",
        "\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "print(max_sequence_len)\n",
        "\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmWHCO0dQGlZ"
      },
      "source": [
        "You can then print some of the examples as a sanity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJtwVB2NbOAP"
      },
      "outputs": [],
      "source": [
        "sentence = corpus[0].split()\n",
        "print(f'sample sentence: {sentence}')\n",
        "\n",
        "token_list = []\n",
        "\n",
        "for word in sentence: \n",
        "  token_list.append(tokenizer.word_index[word])\n",
        "\n",
        "print(token_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMr6kKfzROlW"
      },
      "outputs": [],
      "source": [
        "elem_number = 5\n",
        "\n",
        "print(f'token list: {xs[elem_number]}')\n",
        "print(f'decoded to text: {tokenizer.sequences_to_texts([xs[elem_number]])}')\n",
        "\n",
        "print(f'one-hot label: {ys[elem_number]}')\n",
        "print(f'index of label: {np.argmax(ys[elem_number])}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49Cv68JOakwv"
      },
      "outputs": [],
      "source": [
        "elem_number = 4\n",
        "\n",
        "print(f'token list: {xs[elem_number]}')\n",
        "print(f'decoded to text: {tokenizer.sequences_to_texts([xs[elem_number]])}')\n",
        "\n",
        "print(f'one-hot label: {ys[elem_number]}')\n",
        "print(f'index of label: {np.argmax(ys[elem_number])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKWWUZm5VPG9"
      },
      "source": [
        "## Build and compile the Model\n",
        "\n",
        "Next, you will build and compile the model. We placed some of the hyperparameters at the top of the code cell so you can easily tweak it later if you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9vH8Y59ajYL"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100\n",
        "lstm_units = 150\n",
        "learning_rate = 0.01\n",
        "\n",
        "model = Sequential([\n",
        "          Embedding(total_words, embedding_dim, input_length=max_sequence_len-1),\n",
        "          Bidirectional(LSTM(lstm_units)),\n",
        "          Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy', \n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
        "    metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpI0d9cfR43c"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "From the model summary above, you'll notice that the number of trainable params is much larger than the one in the previous lab. Consequently, that usually means a slower training time. It will take roughly 7 seconds per epoch with the GPU enabled in Colab and you'll reach around 76% accuracy after 100 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nc4zC7C4jJpN"
      },
      "outputs": [],
      "source": [
        "epochs = 1\n",
        "\n",
        "history = model.fit(xs, ys, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"AI_CHAT\")"
      ],
      "metadata": {
        "id": "tAChr2ShxyUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgAzLnLATFts"
      },
      "source": [
        "You can visualize the accuracy below to see how it fluctuates as the training progresses."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/AI_CHAT.zip /content/AI_CHAT"
      ],
      "metadata": {
        "id": "hN-TuvPF6RpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/AI_CHAT.zip\")"
      ],
      "metadata": {
        "id": "sXBnPgei7RP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/AI_CHAT1.zip -d AI_CHAT1"
      ],
      "metadata": {
        "id": "LrcpTaHl79k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "model = tf.keras.models.load_model(\"AI_CHAT1/content/AI_CHAT1\")"
      ],
      "metadata": {
        "id": "-pVPQ3C48f8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = model.layers[0]\n",
        "\n",
        "embedding_weights = embedding_layer.get_weights()[0]\n",
        "\n",
        "print(embedding_weights.shape) \n"
      ],
      "metadata": {
        "id": "M_V5YMn7940m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_word_index = tokenizer.index_word"
      ],
      "metadata": {
        "id": "hve9L-slEMvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "\n",
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for word_num in range(1, total_words):\n",
        "\n",
        "  word_name = reverse_word_index[word_num]\n",
        "\n",
        "  word_embedding = embedding_weights[word_num]\n",
        "\n",
        "  out_m.write(word_name + \"\\n\")\n",
        "\n",
        "  out_v.write('\\t'.join([str(x) for x in word_embedding]) + \"\\n\")\n",
        "\n",
        "out_v.close()\n",
        "out_m.close()\n"
      ],
      "metadata": {
        "id": "nBlQuN8lEPQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import files utilities in Colab\n",
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "\n",
        "else:\n",
        "  files.download('vecs.tsv')\n",
        "  files.download('meta.tsv')\n"
      ],
      "metadata": {
        "id": "gqbOveF7ERUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YXGelKThoTT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gxKIcvGTUnw"
      },
      "source": [
        "## Generating Text\n",
        "\n",
        "Now you can let the model make its own songs or poetry! Because it is trained on a much larger corpus, the results below should contain less repetitions as before. The code below picks the next word based on the highest probability output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Vc6PHgxa6Hm"
      },
      "outputs": [],
      "source": [
        "seed_text = \"जानम देख लो\"\n",
        "\n",
        "next_words = 5\n",
        "\n",
        "for _ in range(next_words):\n",
        "\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\t\n",
        "\tprobabilities = model.predict(token_list)\n",
        "\n",
        "\tpredicted = np.argmax(probabilities, axis=-1)[0]\n",
        "\n",
        "\tif predicted != 0:\n",
        "\t\t\n",
        "\t\toutput_word = tokenizer.index_word[predicted]\n",
        "\n",
        "\t\tseed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHtrtAFAT6tn"
      },
      "source": [
        "Here again is the code that gets the top 3 predictions and picks one at random."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJfzKm-8mVKD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP0--sdMUJ_k"
      },
      "source": [
        "## Wrap Up\n",
        "\n",
        "This lab shows the effect of having a larger dataset to train your text generation model. As expected, this will take a longer time to prepare and train but the output will less likely become repetitive or gibberish. Try to tweak the hyperparameters and see if you get better results. You can also find some other text datasets and use it to train the model here.  "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}